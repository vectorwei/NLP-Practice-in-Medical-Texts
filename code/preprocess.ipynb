{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a51db75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a51db75",
        "outputId": "164be0df-3952-47dc-c230-f712f07db5ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')                                                                                            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bpyoNRYrzosP",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bpyoNRYrzosP"
      },
      "outputs": [],
      "source": [
        "# 进入项目（注意必须是单引号），并切换进mmdetection-2.24.1目录，开始运行代码\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/data/document_parses/pdf_json')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Oc5vrD29e9_h",
      "metadata": {
        "id": "Oc5vrD29e9_h"
      },
      "source": [
        "## Part1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m2gLCAjzuodT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2gLCAjzuodT",
        "outputId": "62c80ff1-756d-4bfe-fedf-1870291c7ce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Jul 31 09:13:38 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71953cbc",
      "metadata": {
        "id": "71953cbc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os, json\n",
        "\n",
        "# this finds our json files\n",
        "word=[]\n",
        "path_to_json =r'/content/drive/MyDrive/data/document_parses/pdf_json'\n",
        "\n",
        "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
        "\n",
        "# here I define my pandas Dataframe with the columns I want to get from the json\n",
        "\n",
        "\n",
        "file = open('test.txt', mode='w',encoding='utf-8')\n",
        "\n",
        "for index, js in enumerate(json_files):\n",
        "\n",
        "    with open(os.path.join(path_to_json, js)) as json_file:\n",
        "\n",
        "        json_text = json.load(json_file)\n",
        "\n",
        "# here you need to know the layout of your json and each json has to have\n",
        "\n",
        "# the same structure (obviously not the structure I have here)\n",
        "        a=len(json_text['body_text'][0])\n",
        "        title = json_text['metadata']['title']\n",
        "        file.write(title)\n",
        "        i = 0\n",
        "        '''while i<a:\n",
        "            \n",
        "            body_text = json_text['body_text'][i][\"text\"]\n",
        "            \n",
        "            file.write(body_text)\n",
        "            i=i+1  '''\n",
        "          \n",
        "\n",
        "\n",
        "           \n",
        "\n",
        "# here I push a list of data into a pandas DataFrame at row given by 'index'\n",
        "\n",
        "       \n",
        "\n",
        "# now that we have the pertinent json data in our DataFrame let's look at it\n",
        "\n",
        "print(word)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a2769ef",
      "metadata": {},
      "source": [
        "### 1.1 split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ff73712",
      "metadata": {},
      "outputs": [],
      "source": [
        "f = open(r\"new1.txt\",encoding=\"utf-8\")\n",
        "a = f.read().split()\n",
        "f.close()\n",
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47c5b261",
      "metadata": {},
      "source": [
        "### 1.2 NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f1d4c65",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "f = open(r\"new1.txt\",encoding=\"utf-8\")\n",
        "b=[]\n",
        "for text in f.readlines():    \n",
        "    a=word_tokenize(text)\n",
        "    b.append(a)\n",
        "f.close()\n",
        "b"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mdIBwvzt3AFk",
      "metadata": {
        "id": "mdIBwvzt3AFk"
      },
      "source": [
        "### 1.3 BPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf5c8535",
      "metadata": {
        "id": "cf5c8535"
      },
      "outputs": [],
      "source": [
        "import re, collections\n",
        "\n",
        "def get_vocab(filename):\n",
        "    vocab = collections.defaultdict(int)\n",
        "    with open(filename, 'r', encoding='utf-8') as fhand:\n",
        "        for line in fhand:\n",
        "            words = line.strip().split()\n",
        "            for word in words:\n",
        "                vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "    return vocab\n",
        "\n",
        "def get_stats(vocab):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "def get_tokens(vocab):\n",
        "    tokens = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        word_tokens = word.split()\n",
        "        for token in word_tokens:\n",
        "            tokens[token] += freq\n",
        "    return tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yr_i_0GZ2nBW",
      "metadata": {
        "id": "yr_i_0GZ2nBW"
      },
      "outputs": [],
      "source": [
        "def get_tokens_from_vocab(vocab):\n",
        "    tokens_frequencies = collections.defaultdict(int)\n",
        "    vocab_tokenization = {}\n",
        "    for word, freq in vocab.items():\n",
        "        # 看vocabulary里面的token频率，相当于上面的code中的tokens去除freq为0的\n",
        "        word_tokens = word.split()\n",
        "        for token in word_tokens:\n",
        "            tokens_frequencies[token] += freq\n",
        "        # vocab和其对应的tokens\n",
        "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
        "    return tokens_frequencies, vocab_tokenization\n",
        "\n",
        "def measure_token_length(token):\n",
        "    \n",
        "    # 如果token最后四个元素是 < / w >\n",
        "    if token[-4:] == '</w>':\n",
        "        # 那就返回除了最后四个之外的长度再加上1(结尾)\n",
        "        return len(token[:-4]) + 1\n",
        "    else:\n",
        "        # 如果这个token里面没有结尾就直接返回当前长度\n",
        "        return len(token)\n",
        "    \n",
        "# 如果vocabulary里面找不到要拆分的词，就根据已经有的token现拆\n",
        "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
        "    \n",
        "    # base case，没词进来了，那拆的结果就是空的\n",
        "    if string == '':\n",
        "        return []\n",
        "    # 已有的sorted tokens没有了，那就真的没这个词了\n",
        "    if sorted_tokens == []:\n",
        "        return [unknown_token] * len(string)\n",
        "\n",
        "    # 记录拆分结果\n",
        "    string_tokens = []\n",
        "    \n",
        "    # iterate over all tokens to find match\n",
        "    for i in range(len(sorted_tokens)):\n",
        "        token = sorted_tokens[i]\n",
        "        \n",
        "        # 自定义一个正则，然后要把token里面包含句号的变成[.]\n",
        "        token_reg = re.escape(token.replace('.', '[.]'))\n",
        "        \n",
        "        # 在当前string里面遍历，找到每一个match token的开始和结束位置，比如string=good，然后token是o，输出[(2,2),(3,3)]?\n",
        "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
        "        # if no match found in the string, go to next token\n",
        "        if len(matched_positions) == 0:\n",
        "            continue\n",
        "        # 因为要拆分这个词，匹配上的token把这个word拆开了，那就要拿到除了match部分之外的substring，所以这里要拿match的start\n",
        "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
        "        substring_start_position = 0\n",
        "        \n",
        "        \n",
        "        # 如果有匹配成功的话，就会进入这个循环\n",
        "        for substring_end_position in substring_end_positions:\n",
        "            # slice for sub-word\n",
        "            substring = string[substring_start_position:substring_end_position]\n",
        "            # tokenize this sub-word with tokens remaining 接着用substring匹配剩余的sorted token，因为刚就匹配了一个\n",
        "            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
        "            # 先把sorted token里面匹配上的记下来\n",
        "            string_tokens += [token]\n",
        "            substring_start_position = substring_end_position + len(token)\n",
        "        # tokenize the remaining string 去除前头的substring，去除已经匹配上的，后面还剩下substring_start_pos到结束的一段substring没看\n",
        "        remaining_substring = string[substring_start_position:]\n",
        "        # 接着匹配\n",
        "        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
        "        break\n",
        "    else:\n",
        "        # return list of unknown token if no match is found for the string\n",
        "        string_tokens = [unknown_token] * len(string)\n",
        "        \n",
        "    return string_tokens\n",
        "\n",
        "\"\"\"\n",
        "EXAMPLE:\n",
        "    token frequency dictionary before sorting: {'natural': 3, 'language':2, 'processing': 4, 'lecture': 4}\n",
        "    sorted tokens: ['processing', 'language', 'lecture', 'natural']\n",
        "    \n",
        "INPUT:\n",
        "    token_frequencies: Dict[str, int] # Counter for token frequency\n",
        "    \n",
        "OUTPUT:\n",
        "    sorted_token: List[str] # Tokens sorted by length and frequency\n",
        "\n",
        "\"\"\"\n",
        "def sort_tokens(tokens_frequencies):\n",
        "    # 对 token_frequencies里面的东西，先进行长度排序，再进行频次，sorted是从低到高所以要reverse\n",
        "    sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item:(measure_token_length(item[0]),item[1]), reverse=True)\n",
        "    \n",
        "    # 然后只要tokens不要频次\n",
        "    sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
        "\n",
        "    return sorted_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rLhMI91V2sP-",
      "metadata": {
        "id": "rLhMI91V2sP-"
      },
      "outputs": [],
      "source": [
        "num_merges = 10000\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(bpe)\n",
        "    if not pairs:\n",
        "        break\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    bpe = merge_vocab(best, bpe)\n",
        "\n",
        "    tokens = get_tokens(bpe)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iO1tdmch3GWA",
      "metadata": {
        "id": "iO1tdmch3GWA"
      },
      "source": [
        "BPE result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb59d895",
      "metadata": {
        "id": "cb59d895"
      },
      "outputs": [],
      "source": [
        "bpe=get_vocab('/content/drive/MyDrive/data/test/new1.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "370ad9fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "370ad9fc",
        "outputId": "62021826-d55e-4c4c-d53f-42234f13baa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('r e c o m b i n a t i o n </w>', 946), ('e v e r y </w>', 5269), ('d a y </w>', 7620), ('a b u n d a n t </w>', 822), ('i n </w>', 700514)]\n"
          ]
        }
      ],
      "source": [
        "print(list(bpe.items())[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MoS_V2_bLhJC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoS_V2_bLhJC",
        "outputId": "39029e65-72ff-4246-fcb8-0049a88fd592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('re co m b in ation</w>', 946), ('e ver y</w>', 5269), ('d a y</w>', 7620), ('ab un d an t</w>', 822), ('in</w>', 700514)]\n"
          ]
        }
      ],
      "source": [
        "print(list(bpe.items())[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FbGeymsdtKS_",
      "metadata": {
        "id": "FbGeymsdtKS_"
      },
      "outputs": [],
      "source": [
        "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(bpe)\n",
        "\n",
        "#sort tokens by length and frequency\n",
        "sorted_tokens = sort_tokens(tokens_frequencies)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OpN0BSeptpja",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpN0BSeptpja",
        "outputId": "b3f68d46-4c45-4f18-faaa-2767df36cc2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1127\n"
          ]
        }
      ],
      "source": [
        "print(len(sorted_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "606abf42",
      "metadata": {},
      "source": [
        "### Save Text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qIsN-OSu-HFI",
      "metadata": {
        "id": "qIsN-OSu-HFI"
      },
      "outputs": [],
      "source": [
        "f=open(\"result.txt\",\"w\")\n",
        " \n",
        "for line in sorted_tokens:\n",
        "    f.write(line+'\\n')\n",
        "f.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "part1.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
