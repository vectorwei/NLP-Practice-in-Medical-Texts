{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce00dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from collections import OrderedDict \n",
    "import math\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea59915a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'paddle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31732\\3553999675.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfluid\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfluid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfluid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdygraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'paddle'"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "from paddle.fluid.dygraph.nn import Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2fe78e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recombination every day  abundant recombination in a virus during a single multi cellular host infectionas increasing numbers of full length viral sequences become available  recombinant or mosaic viruses are being recognized more frequently          recombination events have been demonstrated to be associated with viruses expanding their host range           or increasing their virulence        thus accompanying  or perhaps even being at the origin of  major changes during virus adaptation  it remains unclear  however  whether recombination events represent a highly frequent and significant phenomenon in the everyday life of these viruses why can t i visit  the ethics of visitation restrictions   lessons learned from sarsthe sudden emergence of severe acute respiratory syndrome  sars  in \n"
     ]
    }
   ],
   "source": [
    "def load_text8():\n",
    "    with open(\"new1.txt\", \"r\") as f:\n",
    "        corpus = f.read().strip(\"\\n\")\n",
    "    f.close()\n",
    "    return corpus\n",
    "corpus = load_text8()\n",
    "#打印前800个字符，简要看一下这个语料的样子\n",
    "print(corpus[:800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "852a416d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recombination', 'every', 'day', 'abundant', 'recombination', 'in', 'a', 'virus', 'during', 'a', 'single', 'multi', 'cellular', 'host', 'infectionas', 'increasing', 'numbers', 'of', 'full', 'length', 'viral', 'sequences', 'become', 'available', 'recombinant', 'or', 'mosaic', 'viruses', 'are', 'being', 'recognized', 'more', 'frequently', 'recombination', 'events', 'have', 'been', 'demonstrated', 'to', 'be', 'associated', 'with', 'viruses', 'expanding', 'their', 'host', 'range', 'or', 'increasing', 'their']\n"
     ]
    }
   ],
   "source": [
    "#对语料进行预处理（分词）\n",
    "def data_preprocess(corpus):\n",
    "    #由于英文单词出现在句首的时候经常要大写，所以我们把所有英文字符都转换为小写，\n",
    "    #以便对语料进行归一化处理（Apple vs apple等）\n",
    "    corpus = corpus.strip().lower()\n",
    "    corpus = corpus.split()\n",
    "    return corpus\n",
    "corpus = data_preprocess(corpus)\n",
    "print(corpus[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efdab8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are totoally 308637 different words in the corpus\n",
      "word the, its id 0, its word freq 1363136\n",
      "word of, its id 1, its word freq 1087170\n",
      "word and, its id 2, its word freq 983901\n",
      "word in, its id 3, its word freq 700514\n",
      "word to, its id 4, its word freq 579726\n",
      "word a, its id 5, its word freq 480478\n",
      "word is, its id 6, its word freq 278644\n",
      "word with, its id 7, its word freq 253034\n",
      "word for, its id 8, its word freq 251788\n",
      "word covid, its id 9, its word freq 240991\n",
      "word as, its id 10, its word freq 213808\n",
      "word are, its id 11, its word freq 170359\n",
      "word that, its id 12, its word freq 168587\n",
      "word by, its id 13, its word freq 158399\n",
      "word on, its id 14, its word freq 153602\n",
      "word have, its id 15, its word freq 123685\n",
      "word has, its id 16, its word freq 122842\n",
      "word disease, its id 17, its word freq 121849\n",
      "word from, its id 18, its word freq 116874\n",
      "word sars, its id 19, its word freq 116370\n",
      "word cov, its id 20, its word freq 115233\n",
      "word health, its id 21, its word freq 115016\n",
      "word or, its id 22, its word freq 112237\n",
      "word et, its id 23, its word freq 111887\n",
      "word al, its id 24, its word freq 110760\n",
      "word an, its id 25, its word freq 108500\n",
      "word this, its id 26, its word freq 100544\n",
      "word patients, its id 27, its word freq 99881\n",
      "word coronavirus, its id 28, its word freq 98371\n",
      "word been, its id 29, its word freq 93680\n",
      "word be, its id 30, its word freq 93556\n",
      "word respiratory, its id 31, its word freq 91657\n",
      "word was, its id 32, its word freq 90306\n",
      "word pandemic, its id 33, its word freq 84272\n",
      "word virus, its id 34, its word freq 81212\n",
      "word which, its id 35, its word freq 72996\n",
      "word it, its id 36, its word freq 71861\n",
      "word at, its id 37, its word freq 65392\n",
      "word infection, its id 38, its word freq 65198\n",
      "word severe, its id 39, its word freq 63125\n",
      "word were, its id 40, its word freq 61673\n",
      "word s, its id 41, its word freq 58399\n",
      "word can, its id 42, its word freq 58351\n",
      "word acute, its id 43, its word freq 58116\n",
      "word cases, its id 44, its word freq 57873\n",
      "word such, its id 45, its word freq 57826\n",
      "word these, its id 46, its word freq 56610\n",
      "word syndrome, its id 47, its word freq 54465\n",
      "word more, its id 48, its word freq 53894\n",
      "word their, its id 49, its word freq 50077\n"
     ]
    }
   ],
   "source": [
    "#构造词典，统计每个词的频率，并根据频率将每个词转换为一个整数id\n",
    "def build_dict(corpus):\n",
    "    #首先统计每个不同词的频率（出现的次数），使用一个词典记录\n",
    "    word_freq_dict = dict()\n",
    "    for word in corpus:\n",
    "        if word not in word_freq_dict:\n",
    "            word_freq_dict[word] = 0\n",
    "        word_freq_dict[word] += 1\n",
    "    #将这个词典中的词，按照出现次数排序，出现次数越高，排序越靠前\n",
    "    #一般来说，出现频率高的高频词往往是：I，the，you这种代词，而出现频率低的词，往往是一些名词，如：nlp\n",
    "    word_freq_dict = sorted(word_freq_dict.items(), key = lambda x:x[1], reverse = True)    \n",
    "    #构造3个不同的词典，分别存储，\n",
    "    #每个词到id的映射关系：word2id_dict\n",
    "    #每个id出现的频率：word2id_freq\n",
    "    #每个id到词典映射关系：id2word_dict\n",
    "    word2id_dict = dict()\n",
    "    word2id_freq = dict()\n",
    "    id2word_dict = dict()\n",
    "    #按照频率，从高到低，开始遍历每个单词，并为这个单词构造一个独一无二的id\n",
    "    for word, freq in word_freq_dict:\n",
    "        curr_id = len(word2id_dict)\n",
    "        word2id_dict[word] = curr_id\n",
    "        word2id_freq[word2id_dict[word]] = freq\n",
    "        id2word_dict[curr_id] = word\n",
    "    return word2id_freq, word2id_dict, id2word_dict\n",
    "word2id_freq, word2id_dict, id2word_dict = build_dict(corpus)\n",
    "vocab_size = len(word2id_freq)\n",
    "print(\"there are totoally %d different words in the corpus\" % vocab_size)\n",
    "for _, (word, word_id) in zip(range(50), word2id_dict.items()):\n",
    "    print(\"word %s, its id %d, its word freq %d\" % (word, word_id, word2id_freq[word_id]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97bb5986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26898847 tokens in the corpus\n",
      "[2922, 665, 435, 3234, 2922, 3, 5, 34, 52, 5, 256, 824, 509, 177, 21382, 291, 880, 1, 1123, 1329, 62, 1113, 278, 258, 1690, 22, 7154, 90, 11, 175, 728, 48, 850, 2922, 459, 15, 29, 690, 4, 30, 78, 7, 90, 2913, 49, 177, 359, 22, 291, 49]\n"
     ]
    }
   ],
   "source": [
    "#把语料转换为id序列\n",
    "def convert_corpus_to_id(corpus, word2id_dict):\n",
    "    #使用一个循环，将语料中的每个词替换成对应的id，以便于神经网络进行处理\n",
    "    corpus = [word2id_dict[word] for word in corpus]\n",
    "    return corpus\n",
    "corpus = convert_corpus_to_id(corpus, word2id_dict)\n",
    "print(\"%d tokens in the corpus\" % len(corpus))\n",
    "print(corpus[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d082a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13125621 tokens in the corpus\n",
      "[2922, 665, 435, 3234, 2922, 52, 256, 824, 509, 177, 21382, 291, 1123, 1329, 1113, 278, 1690, 7154, 728, 850, 2922, 690, 78, 2913, 49, 359, 291, 2159, 5718, 2800, 175, 936, 283, 52, 34, 2358, 36, 1565, 2922, 459, 1102, 5, 249, 1120, 1804, 3039, 46, 90, 1645, 211]\n"
     ]
    }
   ],
   "source": [
    "#使用二次采样算法（subsampling）处理语料，强化训练效果\n",
    "def subsampling(corpus, word2id_freq):\n",
    "    \n",
    "    #这个discard函数决定了一个词会不会被替换，这个函数是具有随机性的，每次调用结果不同\n",
    "    #如果一个词的频率很大，那么它被遗弃的概率就很大\n",
    "    def discard(word_id):\n",
    "        return random.uniform(0, 1) < 1 - math.sqrt(\n",
    "            1e-4 / word2id_freq[word_id] * len(corpus))\n",
    "\n",
    "    corpus = [word for word in corpus if not discard(word)]\n",
    "    return corpus\n",
    "corpus = subsampling(corpus, word2id_freq)\n",
    "print(\"%d tokens in the corpus\" % len(corpus))\n",
    "print(corpus[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96fa3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "400000\n",
      "800000\n",
      "1300000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2500000\n"
     ]
    }
   ],
   "source": [
    "#构造数据，准备模型训练\n",
    "#max_window_size代表了最大的window_size的大小，程序会根据max_window_size从左到右扫描整个语料\n",
    "#negative_sample_num代表了对于每个正样本，我们需要随机采样多少负样本用于训练，\n",
    "#一般来说，negative_sample_num的值越大，训练效果越稳定，但是训练速度越慢。 \n",
    "def build_data(corpus, word2id_dict, word2id_freq, max_window_size = 3, \n",
    "               negative_sample_num = 6):\n",
    "    \n",
    "    #使用一个list存储处理好的数据\n",
    "    dataset = []\n",
    "    center_word_idx=0\n",
    "\n",
    "    #从左到右，开始枚举每个中心点的位置\n",
    "    while center_word_idx < len(corpus):\n",
    "        #以max_window_size为上限，随机采样一个window_size，这样会使得训练更加稳定\n",
    "        window_size = random.randint(1, max_window_size)\n",
    "        #当前的中心词就是center_word_idx所指向的词，可以当作正样本\n",
    "        positive_word = corpus[center_word_idx]\n",
    "\n",
    "        #以当前中心词为中心，左右两侧在window_size内的词就是上下文\n",
    "        context_word_range = (max(0, center_word_idx - window_size), min(len(corpus) - 1, center_word_idx + window_size))\n",
    "        context_word_candidates = [corpus[idx] for idx in range(context_word_range[0], context_word_range[1]+1) if idx != center_word_idx]\n",
    "\n",
    "        #对于每个正样本来说，随机采样negative_sample_num个负样本，用于训练\n",
    "        for context_word in context_word_candidates:\n",
    "            #首先把（上下文，正样本，label=1）的三元组数据放入dataset中，\n",
    "            #这里label=1表示这个样本是个正样本\n",
    "            dataset.append((context_word, positive_word, 1))\n",
    "\n",
    "            #开始负采样\n",
    "            i = 0\n",
    "            while i < negative_sample_num:\n",
    "                negative_word_candidate = random.randint(0, vocab_size-1)\n",
    "\n",
    "                if negative_word_candidate is not positive_word:\n",
    "                    #把（上下文，负样本，label=0）的三元组数据放入dataset中，\n",
    "                    #这里label=0表示这个样本是个负样本\n",
    "                    dataset.append((context_word, negative_word_candidate, 0))\n",
    "                    i += 1\n",
    "        \n",
    "        center_word_idx = min(len(corpus) - 1, center_word_idx + window_size)\n",
    "        if center_word_idx == (len(corpus) - 1):\n",
    "            center_word_idx += 1\n",
    "        if center_word_idx % 100000 == 0:\n",
    "            print(center_word_idx)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = build_data(corpus, word2id_dict, word2id_freq)\n",
    "for _, (context_word, target_word, label) in zip(range(50), dataset):\n",
    "    print(\"center_word %s, target %s, label %d\" % (id2word_dict[context_word],\n",
    "                                                   id2word_dict[target_word], label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce26b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造mini-batch，准备对模型进行训练\n",
    "#我们将不同类型的数据放到不同的tensor里，便于神经网络进行处理\n",
    "#并通过numpy的array函数，构造出不同的tensor来，并把这些tensor送入神经网络中进行训练\n",
    "def build_batch(dataset, batch_size, epoch_num):\n",
    "    \n",
    "    #center_word_batch缓存batch_size个中心词\n",
    "    center_word_batch = []\n",
    "    #target_word_batch缓存batch_size个目标词（可以是正样本或者负样本）\n",
    "    target_word_batch = []\n",
    "    #label_batch缓存了batch_size个0或1的标签，用于模型训练\n",
    "    label_batch = []\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        #每次开启一个新epoch之前，都对数据进行一次随机打乱，提高训练效果\n",
    "        random.shuffle(dataset)\n",
    "        \n",
    "        for center_word, target_word, label in dataset:\n",
    "            #遍历dataset中的每个样本，并将这些数据送到不同的tensor里\n",
    "            center_word_batch.append([center_word])\n",
    "            target_word_batch.append([target_word])\n",
    "            label_batch.append(label)\n",
    "\n",
    "            #当样本积攒到一个batch_size后，我们把数据都返回回来\n",
    "            #在这里我们使用numpy的array函数把list封装成tensor\n",
    "            #并使用python的迭代器机制，将数据yield出来\n",
    "            #使用迭代器的好处是可以节省内存\n",
    "            if len(center_word_batch) == batch_size:\n",
    "                yield np.array(center_word_batch).astype(\"int64\"), \\\n",
    "                    np.array(target_word_batch).astype(\"int64\"), \\\n",
    "                    np.array(label_batch).astype(\"float32\")\n",
    "                center_word_batch = []\n",
    "                target_word_batch = []\n",
    "                label_batch = []\n",
    "\n",
    "    if len(center_word_batch) > 0:\n",
    "        yield np.array(center_word_batch).astype(\"int64\"), \\\n",
    "            np.array(target_word_batch).astype(\"int64\"), \\\n",
    "            np.array(label_batch).astype(\"float32\")\n",
    "\n",
    "# for _, batch in zip(range(10), build_batch(dataset, 128, 3)):\n",
    "#     print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f1b38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义skip-gram训练网络结构\n",
    "#这里我们使用的是paddlepaddle的1.8.0版本\n",
    "#一般来说，在使用fluid训练的时候，我们需要通过一个类来定义网络结构，这个类继承了fluid.dygraph.Layer\n",
    "class SkipGram(fluid.dygraph.Layer):\n",
    "    def __init__(self, vocab_size, embedding_size, init_scale=0.1):\n",
    "        #vocab_size定义了这个skipgram这个模型的词表大小\n",
    "        #embedding_size定义了词向量的维度是多少\n",
    "        #init_scale定义了词向量初始化的范围，一般来说，比较小的初始化范围有助于模型训练\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        #使用paddle.fluid.dygraph提供的Embedding函数，构造一个词向量参数\n",
    "        #这个参数的大小为：[self.vocab_size, self.embedding_size]\n",
    "        #数据类型为：float32\n",
    "        #这个参数的名称为：embedding_para\n",
    "        #这个参数的初始化方式为在[-init_scale, init_scale]区间进行均匀采样\n",
    "        self.embedding = Embedding(\n",
    "            size=[self.vocab_size, self.embedding_size],\n",
    "            dtype='float32',\n",
    "            param_attr=fluid.ParamAttr(\n",
    "                name='embedding_para',\n",
    "                initializer=fluid.initializer.UniformInitializer(\n",
    "                    low=-0.5/embedding_size, high=0.5/embedding_size)))\n",
    "\n",
    "        #使用paddle.fluid.dygraph提供的Embedding函数，构造另外一个词向量参数\n",
    "        #这个参数的大小为：[self.vocab_size, self.embedding_size]\n",
    "        #数据类型为：float32\n",
    "        #这个参数的名称为：embedding_para_out\n",
    "        #这个参数的初始化方式为在[-init_scale, init_scale]区间进行均匀采样\n",
    "        #跟上面不同的是，这个参数的名称跟上面不同，因此，\n",
    "        #embedding_para_out和embedding_para虽然有相同的shape，但是权重不共享\n",
    "        self.embedding_out = Embedding(\n",
    "            size=[self.vocab_size, self.embedding_size],\n",
    "            dtype='float32',\n",
    "            param_attr=fluid.ParamAttr(\n",
    "                name='embedding_out_para',\n",
    "                initializer=fluid.initializer.UniformInitializer(\n",
    "                    low=-0.5/embedding_size, high=0.5/embedding_size)))\n",
    "\n",
    "    #定义网络的前向计算逻辑\n",
    "    #center_words是一个tensor（mini-batch），表示中心词\n",
    "    #target_words是一个tensor（mini-batch），表示目标词\n",
    "    #label是一个tensor（mini-batch），表示这个词是正样本还是负样本（用0或1表示）\n",
    "    #用于在训练中计算这个tensor中对应词的同义词，用于观察模型的训练效果\n",
    "    def forward(self, center_words, target_words, label):\n",
    "        #首先，通过embedding_para（self.embedding）参数，将mini-batch中的词转换为词向量\n",
    "        #这里center_words和eval_words_emb查询的是一个相同的参数\n",
    "        #而target_words_emb查询的是另一个参数\n",
    "        center_words_emb = self.embedding(center_words)\n",
    "        target_words_emb = self.embedding_out(target_words)\n",
    "\n",
    "        #center_words_emb = [batch_size, embedding_size]\n",
    "        #target_words_emb = [batch_size, embedding_size]\n",
    "        #我们通过点乘的方式计算中心词到目标词的输出概率，并通过sigmoid函数估计这个词是正样本还是负样本的概率。\n",
    "        word_sim = fluid.layers.elementwise_mul(center_words_emb, target_words_emb)\n",
    "        word_sim = fluid.layers.reduce_sum(word_sim, dim = -1)\n",
    "        word_sim = fluid.layers.reshape(word_sim, shape=[-1])\n",
    "        pred = fluid.layers.sigmoid(word_sim)\n",
    "\n",
    "        #通过估计的输出概率定义损失函数，注意我们使用的是sigmoid_cross_entropy_with_logits函数\n",
    "        #将sigmoid计算和cross entropy合并成一步计算可以更好的优化，所以输入的是word_sim，而不是pred\n",
    "        \n",
    "        loss = fluid.layers.sigmoid_cross_entropy_with_logits(word_sim, label)\n",
    "        loss = fluid.layers.reduce_mean(loss)\n",
    "\n",
    "        #返回前向计算的结果，飞桨会通过backward函数自动计算出反向结果。\n",
    "        return pred, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c066b0e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2id_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31732\\4262220524.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'embedding.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mget_cos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"king\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"queen\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mget_cos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"she\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"her\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mget_cos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"topic\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"theme\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31732\\4262220524.py\u001b[0m in \u001b[0;36mget_cos\u001b[1;34m(query1_token, query2_token, embed)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_cos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery1_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery2_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword2id_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mquery1_token\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword2id_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mquery2_token\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word2id_dict' is not defined"
     ]
    }
   ],
   "source": [
    "#定义一个使用word-embedding计算cos的函数\n",
    "def get_cos(query1_token, query2_token, embed):\n",
    "    W = embed\n",
    "    x = W[word2id_dict[query1_token]]\n",
    "    y = W[word2id_dict[query2_token]]\n",
    "    cos = np.dot(x, y) / np.sqrt(np.sum(y * y) * np.sum(x * x) + 1e-9)\n",
    "    flat = cos.flatten()\n",
    "    print(\"单词1 %s 和单词2 %s 的cos结果为 %f\" %(query1_token, query2_token, cos) )\n",
    "\n",
    "embedding_matrix = np.load('embedding.npy') \n",
    "get_cos(\"king\",\"queen\",embedding_matrix)\n",
    "get_cos(\"she\",\"her\",embedding_matrix)\n",
    "get_cos(\"topic\",\"theme\",embedding_matrix)\n",
    "get_cos(\"woman\",\"game\",embedding_matrix)\n",
    "get_cos(\"one\",\"name\",embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b02987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09093317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
